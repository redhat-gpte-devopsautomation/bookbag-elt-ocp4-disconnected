:GUID: %guid%
:OSP_DOMAIN: %dns_zone%
:GITLAB_URL: %gitlab_url%
:GITLAB_USERNAME: %gitlab_username%
:GITLAB_PASSWORD: %gitlab_password%
:GITLAB_HOST: %gitlab_hostname%
:TOWER_URL: %tower_url%
:TOWER_ADMIN_USER: %tower_admin_user%
:TOWER_ADMIN_PASSWORD: %tower_admin_password%
:SSH_COMMAND: %ssh_command%
:SSH_PASSWORD: %ssh_password%
:VSCODE_UI_URL: %vscode_ui_url%
:VSCODE_UI_PASSWORD: %vscode_ui_password%
:organization_name: Default
:gitlab_project: ansible/gitops-lab
:project_prod: Project gitOps - Prod
:project_test: Project gitOps - Test
:inventory_prod: GitOps inventory - Prod Env
:inventory_test: GitOps inventory - Test Env
:credential_machine: host_credential
:credential_git: gitlab_credential
:credential_git_token: gitlab_token 
:credential_openstack: cloud_credential
:jobtemplate_prod: App deployer - Prod Env
:jobtemplate_test: App deployer - Test Env
:source-linenums-option:        
:markup-in-source: verbatim,attributes,quotes
:show_solution: true
:catalog_name: OpenShift 4 Advanced Infra Deploy ILT
:course_name: Advanced Red Hat OpenShift Container Platform Deployment and Management
:opentlc_portal: link:https://labs.opentlc.com/[OPENTLC lab portal^]
:opentlc_account_management: link:https://www.opentlc.com/account/[OPENTLC Account Management page^]
:opentlc_catalog_name: OPENTLC OpenShift 4 Labs
:opentlc_catalog_item_name_aws: OpenShift 4 Advanced Infra Deploy ILT
:ocp4_docs: link:https://docs.openshift.com/container-platform/4.11/welcome/index.html[OpenShift Container Platform Documentation]

== OpenShift Installation

A lot of work is now behind you.
In the previous sections, you had to create and configure a lot of different things and you are not done yet!
Think about what you have done and what you still need to create in order to have a functional OpenShift cluster.
You essentially have:

* Several installation files on your bastion
* Container images cloned to your local container registry
* An `ignition` file to bootstrap your cluster

With all of these, you can start creating the actual servers that will make up your OpenShift 4 cluster.
Recall the process to `bootstrap` an OpenShift 4 cluster:

. The `bootstrap` machine boots and starts hosting the remote resources required for the master machines to boot.
. The `master` machines fetch the remote resources from the bootstrap machine and finish booting.
. The `master` machines use the bootstrap machine to form an etcd cluster.
. The `bootstrap` machine starts a temporary Kubernetes control plane using the new etcd cluster.
. The `temporary control plane` schedules the production control plane to the master machines.
. The `temporary control plane` shuts down and passes control to the production control plane.
. The `bootstrap` machine injects OpenShift Container Platform components into the `production control plane`.
. The installation program shuts down the bootstrap machine.
. The control plane sets up the `worker nodes`.
. The control plane installs additional services in the form of a set of `Operators`.

The steps above can be completely automated using the IPI method.
These steps can also be accomplished using the UPI method, which you will continue with in the following sections.

Before moving on, take a few minutes to review this and think about the steps above that you will need to manually complete and which steps the `installer` and `bootkube` program will handle for you.
Take a moment to go back and review the <<Environment Overview>> for a list of components that are provided to you in this lab environment.
You will use these as you build your OpenShift cluster.
Also, review the link:https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-restricted-networks-aws-installer-provisioned.html[OpenShift Documentation^] for additional details on what you need to install an OpenShift 4 cluster on AWS.

=== Create Environment Variables

. Environment Variables to use throughout the installation process (make sure to replace NNNN in your Base Domain with the actual sandbox number for your environment).
+
[CAUTION]
Make sure to replace the `NNNN` for the sandbox in the second command below!
+
[source,bash]
----
ansible localhost -m lineinfile -a "path=$HOME/.bashrc regexp='^export CLUSTER_NAME' line='export CLUSTER_NAME=cluster-$GUID'"

ansible localhost -m lineinfile -a "path=$HOME/.bashrc regexp='^export BASE_DOMAIN' line='export BASE_DOMAIN=sandboxNNNN.opentlc.com'"

source $HOME/.bashrc
----

////
. Clone the resources repo into your $HOME/resources directory.
This has the AWS cloudformation automated template generation files.
+
----
rm -rf ~/resources
git clone https://github.com/newgoliath/aws_upi_cloudformations $HOME/resources/
----
////

. Add the INFRA_ID, CLUSTER_NAME, and BASE_DOMAIN to your `~/resources/cluster_vars.yaml` file.
It'll be used to deploy AWS CloudFormations templates.
+
[source,bash]
----
touch $HOME/resources/cluster_vars.yaml

ansible localhost -m lineinfile -a "path=$HOME/resources/cluster_vars.yaml regexp='^INFRA_ID' line='INFRA_ID: $INFRA_ID'"

ansible localhost -m lineinfile -a "path=$HOME/resources/cluster_vars.yaml regexp='^CLUSTER_NAME' line='CLUSTER_NAME: $CLUSTER_NAME'"

ansible localhost -m lineinfile -a "path=$HOME/resources/cluster_vars.yaml regexp='^BASE_DOMAIN' line='BASE_DOMAIN: $BASE_DOMAIN'"

cat ~/resources/cluster_vars.yaml

----

+
.Sample Output
[source,texinfo]
----
[...]
INFRA_ID: cluster-67e1-jgbdh
CLUSTER_NAME: cluster-67e1
BASE_DOMAIN: sandbox644.opentlc.com
----

=== Create AWS VPC

Discussion

. Examine the parameters file, `~/resources/aws_upi_vpc_parameters.json`.
+
NOTE: You do not need to change anything

The STACK_NAME below will be your $INFRA_ID already created by the installer and now in your environment variables.

. Run the cloudformations template.
+
[source,bash]
----
aws cloudformation create-stack --stack-name ${INFRA_ID}-vpc \
    --template-body file://~/resources/aws_upi_vpc_template.yaml \
    --parameters file://~/resources/aws_upi_vpc_parameters.json
----

. Watch for status until the status is *CREATE_COMPLETE* (hit `Ctrl-C` to stop the watch). Be patient, this takes a while:
+
[source,bash]
----
watch -n 5 aws cloudformation describe-stacks --stack-name $INFRA_ID-vpc --query Stacks[0].StackStatus
----
+
[NOTE]
If you get an error (like *ROLLBACK_COMPLETE*) delete your stack (`aws cloudformation delete-stack --stack-name ${INFRA_ID}-vpc`) and double check your parameters. Then try again.

. You will need the values for `VpcId`, `PublicSubnetId`, `PrivateSubnetIds` in the next step.
Get the values and put them in `~/resources/vpc.txt` and `~/resources/cluster_vars.yaml`
+
[source,sh]
----
aws cloudformation describe-stacks --stack-name $INFRA_ID-vpc --query Stacks[0].Outputs[].[OutputKey,OutputValue] --output text | sed 's_\s_: _g' |  tee ~/resources/vpc.txt >> ~/resources/cluster_vars.yaml

cat ~/resources/cluster_vars.yaml
----
+
.Sample Output
[source,texinfo]
----
[...]
PrivateSubnetIds: subnet-013928010190417b3
PublicSubnetIds: subnet-0d3e1a3a91f866e3a
VpcId: vpc-008a7ee8fce4b4799
----

=== Create AWS DNS and Load Balancers

Discussion

. Get your HostedZoneID for your base domain that you use in the `install-config.yaml` file.
For example: `sandbox2813.opentlc.com`.
+
[source,bash]
----
aws route53 list-hosted-zones-by-name --dns-name $BASE_DOMAIN --query "HostedZones[?starts_with(Name, 'sandbox')].[Id,Name]" --output text
----
+
.Sample Output:
[source,json]
----
/hostedzone/Z03415621TPFS7UFRJXBT<1>      sandbox2813.opentlc.com.<2>
----
<1> The string Z3S2I1584B1CPB is the HostedZoneID
<2> This is an example base domain

. Add the HostedZoneId to the `~/resources/cluster_vars.yaml`
+
[source,bash]
----
aws route53 list-hosted-zones-by-name --dns-name $BASE_DOMAIN --query "HostedZones[?starts_with(Name, 'sandbox')].[Id]" --output text | awk -F "/" '{print $3}' | xargs -I{} ansible localhost -m lineinfile "-a path=$HOME/resources/cluster_vars.yaml regexp='^HostedZoneId' line='HostedZoneId: {}'"
----

. Process the templates to generate `~/resources/aws_upi_route53_parameters.json`.
+
[source,bash]
----
cd ~/resources
ansible-playbook ./process.yaml
----
+
NOTE: Only two parameters files will succeed when processing this command: `aws_upi_route53_parameters.json.j2` and `aws_upi_sec_parameters.json.j2`. This is expected. You will run this command again when you have more information available later.

. Examine the file `~/resources/aws_upi_route53_parameters.json`.
+
[source,sh]
----
cat ~/resources/aws_upi_route53_parameters.json
----
+
.Sample Output:
[source,json]
----
[
  {
    "ParameterKey": "ClusterName",
    "ParameterValue": "cluster-smp1235"
  },
  {
    "ParameterKey": "InfrastructureName",
    "ParameterValue": "cluster-smp1235-m9z8h"
  },
  {
    "ParameterKey": "HostedZoneId",
    "ParameterValue": "Z03415621TPFS7UFRJXBT"
  },
  {
    "ParameterKey": "HostedZoneName",
    "ParameterValue": "sandbox2813.opentlc.com"
  },
  {
    "ParameterKey": "PublicSubnets",
    "ParameterValue": "subnet-086d44765f6a6f5c4"
  },
  {
    "ParameterKey": "PrivateSubnets",
    "ParameterValue": "subnet-00c8befe9c067f2f4"
  },
  {
    "ParameterKey": "VpcId",
    "ParameterValue": "vpc-07495b25035849d10"
  }
]
----

. Run the cloudformations template to update DNS and create ELBs
+
[source,bash]
----
aws cloudformation create-stack --stack-name ${INFRA_ID}-dns \
    --template-body file://~/resources/aws_upi_route53_template.yaml \
    --parameters file://~/resources/aws_upi_route53_parameters.json \
    --capabilities CAPABILITY_NAMED_IAM
----

. Check the status until the status is *CREATE_COMPLETE*. Again this will take a while.
+
[source,bash]
----
watch -n 5 aws cloudformation describe-stacks --stack-name ${INFRA_ID}-dns --query Stacks[0].StackStatus
----
+
[NOTE]
If you get an error (like *ROLLBACK_COMPLETE*) delete your stack (`aws cloudformation delete-stack --stack-name ${INFRA_ID}-dns`) and double check your parameters. Then try again.

. Retrieve the output of the stack. You'll need these variables later.
+
[source,sh]
----
aws cloudformation describe-stacks --stack-name ${INFRA_ID}-dns --query Stacks[0].Outputs[].[OutputKey,OutputValue] --output text | sed 's_\s_: _g' | tee ~/resources/dns.txt >> ~/resources/cluster_vars.yaml

cat ~/resources/cluster_vars.yaml
----
+
.Sample Output:
[source,text]
----
[...]
ExternalApiTargetGroupArn: arn:aws:elasticloadbalancing:us-east-2:888069851216:targetgroup/clust-Exter-KIJHW5I3K7JA/f11ef32f4cda5ad2
InternalApiTargetGroupArn: arn:aws:elasticloadbalancing:us-east-2:888069851216:targetgroup/clust-Inter-E63NCXWUJA41/d272758b4f5ca1ac
ApiServerDnsName: api-int.cluster-911d.sandbox702.opentlc.com
PrivateHostedZoneId: Z0116250HTJAKCYZJDL9
InternalApiLoadBalancerName: net/cluster-911d-2nrj4-int/47596a07c3d0afe6
RegisterNlbIpTargetsLambda: arn:aws:lambda:us-east-2:888069851216:function:cluster-911d-2nrj4-dns-RegisterNlbIpTargets-15S4NPOGUYJ2B
InternalServiceTargetGroupArn: arn:aws:elasticloadbalancing:us-east-2:888069851216:targetgroup/clust-Inter-11MNRLIJUNB50/c007c51a975ccf2f
ExternalApiLoadBalancerName: net/cluster-911d-2nrj4-ext/053207a632600374
----

=== Setup VPC Peering and Routes

==== Setup VPC Peering

In order for the OpenShift nodes to be able to pull container images from the custom container registry you need to connect the VPCs for OpenShift and the one that the utility vm is in.

. Retrieve the Route53 hosted zones and find the *Id* for name *$GUID.internal*:
+
[source,sh]
----
aws route53 list-hosted-zones --query "HostedZones[].[Id, Name]" --output text
----

. Remind yourself of the two VPCs that exist in your environment. There is the one that you created with a CIDR of *10.0.0.0/16*  and another one that contains your bastion and utilityvm with a CIDR of *192.168.0.0/16*
+
[source,sh]
----
aws ec2 describe-vpcs --query "Vpcs[].[VpcId,CidrBlock]" --output text
----

. Add the hosted zone (GUID.internal) to the new VPC (the one with CIDR 10.0.0.0/16). *Replace the examples in the command below with your specific values!*
+
[source,bash]
----
aws route53 associate-vpc-with-hosted-zone \
--hosted-zone-id="Z034707520LL5ZQD8BHO3" \
--vpc VPCRegion=us-east-2,VPCId=vpc-07495b25035849d10
----

. Set a VPC Peering connection by connecting the two VPCs
.. Create a peering request passing the two VPC ids. *Replace the examples in the command below with your specific values!*
+
[source,bash]
----
aws ec2 create-vpc-peering-connection --peer-vpc-id vpc-07495b25035849d10 --vpc-id vpc-0ba9dafaa4457a3fb
----

.. Get the peering request ID
+
[source,bash]
----
aws ec2 describe-vpc-peering-connections --query VpcPeeringConnections[0].VpcPeeringConnectionId
----
+
.Sample Output
[source,text]
----
"pcx-062b90bf1c68f8542"
----

.. Approve the Peering Request. *Replace the examples in the command below with your specific values!*
+
[source,bash]
----
aws ec2 accept-vpc-peering-connection --vpc-peering-connection-id pcx-062b90bf1c68f8542
----

==== Add routes between VPCs

. For each of the VPCs, get all the RouteTableIds, VpcIds, and First Dests
+
[source,bash]
----
aws ec2 describe-route-tables --query 'RouteTables[*].{TABLE:RouteTableId,VPC:VpcId,DEST:Routes[*].DestinationCidrBlock}' --output text
----
// aws ec2 describe-vpcs --query "Vpcs[].[VpcId]" --output text | xargs -I{} aws ec2 describe-route-tables --filters "Name=vpc-id,Values={}" --query 'RouteTables[*].{TABLE:RouteTableId,VPC:VpcId,DEST:Routes[*].DestinationCidrBlock}'

+
.Sample Output
[source,yaml]
----
rtb-0d89f211252ea274a	vpc-0ba9dafaa4457a3fb
DEST	192.168.0.0/16
DEST	0.0.0.0/0
rtb-05188fb6b3f5e1d22	vpc-07495b25035849d10
DEST	10.0.0.0/16
rtb-0aaae49ff40b9ac67	vpc-07495b25035849d10
DEST	10.0.0.0/16
DEST	0.0.0.0/0
rtb-068ca21a2f9fa1452	vpc-07495b25035849d10
DEST	10.0.0.0/16
DEST	0.0.0.0/0
rtb-05d531ce8482266a7	vpc-0ba9dafaa4457a3fb
DEST	192.168.0.0/16
----

. What was the Peering ID?
+
[source,bash]
----
aws ec2 describe-vpc-peering-connections --query "VpcPeeringConnections[0].VpcPeeringConnectionId"
----
+
.Sample Output:
[source,text]
----
"pcx-062b90bf1c68f8542"
----

. For all the routes with DEST:192.168.0.0, add a route to 10.0.0.0/16, and vice versa. Include the PeeringId, the destination cidr, and the route table ID. *Replace the examples in the command below with your specific values!*
+
[source,bash]
----
aws ec2 create-route --vpc-peering-connection-id pcx-062b90bf1c68f8542 --destination-cidr-block "10.0.0.0/16" --route-table-id rtb-0d89f211252ea274a
{
    "Return": true
}
aws ec2 create-route --vpc-peering-connection-id pcx-062b90bf1c68f8542 --destination-cidr-block "10.0.0.0/16" --route-table-id rtb-05d531ce8482266a7
{
    "Return": true
}
aws ec2 create-route --vpc-peering-connection-id pcx-062b90bf1c68f8542 --destination-cidr-block "192.168.0.0/16" --route-table-id rtb-05188fb6b3f5e1d22
{
    "Return": true
}
aws ec2 create-route --vpc-peering-connection-id pcx-062b90bf1c68f8542 --destination-cidr-block "192.168.0.0/16" --route-table-id rtb-0aaae49ff40b9ac67
{
    "Return": true
}
aws ec2 create-route --vpc-peering-connection-id pcx-062b90bf1c68f8542 --destination-cidr-block "192.168.0.0/16" --route-table-id rtb-068ca21a2f9fa1452
{
    "Return": true
}
----
+
Your routes should now work between all the subnets across both VPCs.
You'll be testing this later.

. Check your work by examining the tables again. For each of the VPCs, get all the RouteTableIds, VpcIds, and First Dests
+
[source,bash]
----
aws ec2 describe-route-tables --query 'RouteTables[*].{TABLE:RouteTableId,VPC:VpcId,DEST:Routes[*].DestinationCidrBlock}' --output text
----
+
.Sample Output
[source,yaml]
----
rtb-0d89f211252ea274a	vpc-0ba9dafaa4457a3fb
DEST	192.168.0.0/16
DEST	10.0.0.0/16
DEST	0.0.0.0/0
rtb-05188fb6b3f5e1d22	vpc-07495b25035849d10
DEST	192.168.0.0/16
DEST	10.0.0.0/16
rtb-0aaae49ff40b9ac67	vpc-07495b25035849d10
DEST	192.168.0.0/16
DEST	10.0.0.0/16
DEST	0.0.0.0/0
rtb-068ca21a2f9fa1452	vpc-07495b25035849d10
DEST	192.168.0.0/16
DEST	10.0.0.0/16
DEST	0.0.0.0/0
rtb-05d531ce8482266a7	vpc-0ba9dafaa4457a3fb
DEST	192.168.0.0/16
DEST	10.0.0.0/16
----

=== Create AWS Security Groups

. You already have a Cloud Formation template file with associated parameters available. Examine the parameters:
+
[source,sh]
----
cat ~/resources/aws_upi_sec_parameters.json
----
+
.Sample Output
[source,texinfo]
----
[
  {
    "ParameterKey": "InfrastructureName",
    "ParameterValue": "cluster-smp1235-m9z8h"
  },
  {
    "ParameterKey": "VpcCidr",
    "ParameterValue": "10.0.0.0/16"
  },
  {
    "ParameterKey": "PrivateSubnets",
    "ParameterValue": "subnet-00c8befe9c067f2f4"
  },
  {
    "ParameterKey": "VpcId",
    "ParameterValue": "vpc-07495b25035849d10"
  }
]
----

. Run the cloudformation template.
+
[source,bash]
----
aws cloudformation create-stack --stack-name ${INFRA_ID}-sec \
    --template-body file://~/resources/aws_upi_sec_template.yaml \
    --parameters file://~/resources/aws_upi_sec_parameters.json \
    --capabilities CAPABILITY_NAMED_IAM
----

. Watch until the stack has been created successfully (*CREATE_COMPLETE*).
+
[source,sh]
----
watch -n 5 aws cloudformation describe-stacks --stack-name $INFRA_ID-sec --query "Stacks[].StackStatus"
----

. Get stack creation output, and add the values to `cluster_vars.yaml`
+
[source,bash]
----
aws cloudformation describe-stacks --stack-name ${INFRA_ID}-sec --query Stacks[0].Outputs[].[OutputKey,OutputValue] --output text | sed 's_\s_: _g' | tee ~/resources/sec.txt >> ~/resources/cluster_vars.yaml

cat ~/resources/cluster_vars.yaml
----
+
.Sample Output:
[source,json]
----
[...]
MasterSecurityGroupId: sg-05b89f8e84d01e091
MasterInstanceProfile: cluster-911d-2nrj4-sec-MasterInstanceProfile-ER7L9QV126UU
WorkerSecurityGroupId: sg-0c57bc6e97dc6f1b8
WorkerInstanceProfile: cluster-911d-2nrj4-sec-WorkerInstanceProfile-160DIX0C0ESYG
----

. Run the template processor to process the rest of the templates. This time it should succeed (you will see three files changed and the two we did earlier not touched).
+
[source,bash]
----
cd $HOME/resources

ansible-playbook ./process.yaml
----
+
.Sample Output
[source,texinfo]
----
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'

PLAY [localhost] ***************************************************************************************************************************

TASK [Gathering Facts] *********************************************************************************************************************
ok: [localhost]

TASK [Process all the templates with cluster_vars.yaml] ************************************************************************************
changed: [localhost] => (item=/home/wkulhane-redhat.com/resources/aws_upi_bootstrap_parameters.json.j2)
changed: [localhost] => (item=/home/wkulhane-redhat.com/resources/aws_upi_control_plane_parameters.json.j2)
ok: [localhost] => (item=/home/wkulhane-redhat.com/resources/aws_upi_route53_parameters.json.j2)
ok: [localhost] => (item=/home/wkulhane-redhat.com/resources/aws_upi_sec_parameters.json.j2)
changed: [localhost] => (item=/home/wkulhane-redhat.com/resources/aws_upi_worker_parameters.json.j2)

PLAY RECAP *********************************************************************************************************************************
localhost                  : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
----

=== Prep for Bootstrap

You are ready to start creating servers in the AWS environment.
What is the first thing you need to start an OpenShift 4 cluster once you have generated all of your installation artifacts in the previous section?
Think about the bootstrap process you learned about.

It all begins with a `bootstrap` node.
It is on that `bootstrap` node where the `bootkube` process runs and starts up a temporary Kubernetes control plane.
Of course, to do that, it also needs `etcd`.
That is provided by the `master` nodes, which will come a little bit later.

Since your cluster and all of your servers will be running on a private network inside of AWS, you do need to provide a way to access them from outside of the AWS cluster.
You will need a way for both API and application ingress traffic to make it into your cluster.
It is best to do this before the installation, so that the installation process can take advantage of these being complete.

NOTE: All of these tasks should be done from your `bastion` while logged in as your 'lab-user' username.

==== *RHCOS AMIs*

In order to create an Instance in AWS you will need to know the specific Instance AMI (ID) for the correct CoreOS image in the correct AWS region.

You can find all the RHCOS AMIs in the OpenShift documentation:
link:https://docs.openshift.com/container-platform/4.11/installing/installing_aws/installing-restricted-networks-aws.html#installation-aws-user-infra-rhcos-ami_installing-restricted-networks-aws[RHCOS AMIs for the AWS infrastructure^]

Here's the one you'll be using.

`us-east-2` `ami-026e5701f495c94a2`

==== S3 Bucket for Bootstrap Ignition files

The ignition file for the bootstrap VM is too big to pass as a parameter. Therefore on AWS you use S3 object storage to pass the ignition file to the bootstrap VM.

. Make a bucket for the `bootstrap.ign` file, upload and verify.
+
[source,bash]
----
aws s3 mb s3://${CLUSTER_NAME}-infra

aws s3 cp $HOME/aws-upi/bootstrap.ign s3://${CLUSTER_NAME}-infra/bootstrap.ign

aws s3 ls s3://${CLUSTER_NAME}-infra/
----

=== Create Bootstrap Instance

. You already created the parameters file for the bootstrap cloud formation template. Examine it:
+
[source,sh]
----
cat ~/resources/aws_upi_bootstrap_parameters.json
----
+
.Sample Output
[source,texinfo]
----
[
  {
    "ParameterKey": "InfrastructureName",
    "ParameterValue": "cluster-f931-hcqsw"
  },
  {
    "ParameterKey": "RhcosAmi",
    "ParameterValue": "ami-026e5701f495c94a2"
  },
  {
    "ParameterKey": "AllowedBootstrapSshCidr",
    "ParameterValue": "0.0.0.0/0"
  },
  {
    "ParameterKey": "PublicSubnet",
    "ParameterValue": "subnet-0cf45b7c3055040cc"
  },
  {
    "ParameterKey": "MasterSecurityGroupId",
    "ParameterValue": "sg-0c4535a906309be2c"
  },
  {
    "ParameterKey": "VpcId",
    "ParameterValue": "vpc-0121d686610261b50"
  },
  {
    "ParameterKey": "BootstrapIgnitionLocation",
    "ParameterValue": "s3://cluster-f931-infra/bootstrap.ign"
  },
  {
    "ParameterKey": "AutoRegisterELB",
    "ParameterValue": "yes"
  },
  {
    "ParameterKey": "RegisterNlbIpTargetsLambdaArn",
    "ParameterValue": "arn:aws:lambda:us-east-2:766853685129:function:cluster-f931-hcqsw-dns-RegisterNlbIpTargets-14T58Q6RN50K8"
  },
  {
    "ParameterKey": "ExternalApiTargetGroupArn",
    "ParameterValue": "arn:aws:elasticloadbalancing:us-east-2:766853685129:targetgroup/clust-Exter-VO3K0GRNW8DG/20b053fc514d893b"
  },
  {
    "ParameterKey": "InternalApiTargetGroupArn",
    "ParameterValue": "arn:aws:elasticloadbalancing:us-east-2:766853685129:targetgroup/clust-Inter-DSHMB0DISS79/d1e411edbe5b0526"
  },
  {
    "ParameterKey": "InternalServiceTargetGroupArn",
    "ParameterValue": "arn:aws:elasticloadbalancing:us-east-2:766853685129:targetgroup/clust-Inter-D39B0FPDQAMA/7e5335428bbf6b62"
  }
]
----

. Create the stack for the bootstrap instance
+
[source,bash]
----
aws cloudformation create-stack --stack-name ${INFRA_ID}-bootstrap \
    --template-body file://~/resources/aws_upi_bootstrap_template.yaml \
    --parameters file://~/resources/aws_upi_bootstrap_parameters.json \
    --capabilities CAPABILITY_NAMED_IAM
----

. Watch for status until the status is *CREATE_COMPLETE* (hit `Ctrl-C` to stop the watch). Once again this will take a while.
+
[source,sh]
----
watch -n 5 aws cloudformation describe-stacks --stack-name ${INFRA_ID}-bootstrap --query "Stacks[].StackStatus"
----
+
[TIP]
====
If you want to find out what is currently being created watch the status of each resource in the stack:

[source,bash]
----
watch aws cloudformation describe-stack-events --stack-name $INFRA_ID-bootstrap --max-items 2

# or

aws cloudformation describe-stack-events --stack-name ${INFRA_ID}-bootstrap
----
====

. Once the stack has been created successfully get the Stack outputs:
+
[source,bash]
----
aws cloudformation describe-stacks --stack-name ${INFRA_ID}-bootstrap --query "Stacks[0].Outputs" --output text
----
+
.Sample Output:
[source,text]
----
Bootstrap Instance ID.  BootstrapInstanceId     i-09c2ea3bf4e9ccf75
The bootstrap node public IP address.   BootstrapPublicIp       18.216.100.168
The bootstrap node private IP address.  BootstrapPrivateIp      10.0.0.80 <1>
----
<1> This is the privte IP address of the bootstrap node.


=== Verify Bootstrap Instance Startup and Logs

. At this point, your OpenShift cluster has started the bootstrapping process.
It is a good idea to make sure that things are running as expected on the `bootstrap` server before you move forward with the next steps.
SSH into your `bootstrap` server from your `bastion` server.

. From the previous command determine the *private* IP Address of your Bootstrap Node.
You can reach that private IP address from your bastion because of the VPC Peering you set up earlier.

. SSH into the Bootstrap VM - note it may take a while for the bootstrap node to accept SSH requests.
+
[source,sh]
----
ssh -i ~/.ssh/${GUID}key.pem core@10.0.10.80
----

. Once logged into the `bootstrap` server, you can see what is happening by looking at the logs for the `bootkube` service.
This process is what orchestrates the installation of the OpenShift components.
You should see the service waiting for an `etcd` cluster, which cannot be created yet because you have not added any `master` nodes to your environment yet.
+
[source,sh]
----
journalctl -b -f -u release-image.service -u bootkube.service
----
+
TIP: If you don't see your `bootkube.service` starting, check the logs of the `release-image.service` to ensure that the image download has started.
You can do this by running `journalctl -u release-image.service`.

. Wait until there is no new output in the log.
Your bootstrap node is up and running and waiting for control plane nodes to be added, which you will do in the next section.
. You may exit out of the bootstrap node (`Ctrl-C` to stop watching the log, `Ctrl-D` or `exit` to exit back to the bastion.)

=== Create Control Plane (master) Instances

The first phase of the bootstrapping process is complete if you've made it this far.
You have a bootstrap node running and you have verified that the `bootkube.service` is running.

What is it waiting for?

It is waiting for `control plane` (master) nodes.
Your `control plane` nodes will run the production control plane, including `etcd`, which is needed first for the bootstrap process to continue.

In this section, you will create three `control plane` nodes.
You should follow the same process you have used in the previous section.

WARNING: You must manually add the certificates to the Control Plane and Worker nodes.

. Add the certificates from the master.ign file to the control plane parameters file.
.. Copy your master cert to your clipboard
+
[source,bash]
----
jq .ignition.security.tls.certificateAuthorities[0].source ~/aws-upi/master.ign
----
.. Paste your master cert to replace the `XXXX` in the file `~/resources/aws_upi_control_plane_parameters.json`. Examine the other parameters to the cloud formation template but don't change them.
+
.Sample Output:
[source,text]
----
  {
    "ParameterKey": "CertificateAuthorities",
    "ParameterValue": "XXXX"
  },
----

. How will these `control plane` nodes know how to join the cluster and become the control plane?
Look at the `IgnitionLocation` parameter that you set for your Cloud Formation stack.
It tells `RHCOS` where to get its configuration from.
+
This DNS name is currently mapped to the `bootstrap` node.
This means that these new `control plane` nodes will boot and get their full configuration from the `bootstrap` node.
This is different than what you had to do for the `bootstrap` node, where you had to host a much larger `ignition` file in an S3 bucket.

. Create the control plane nodes by executing the cloudformations stack.
+
[source,bash]
----
aws cloudformation create-stack --stack-name ${INFRA_ID}-control-plane \
    --template-body file://~/resources/aws_upi_control_plane_template.yaml \
    --parameters file://~/resources/aws_upi_control_plane_parameters.json
----

. Once again watch until the stack status shows *CREATE_COMPLETE*:
+
[source,sh]
----
watch -n 5 aws cloudformation describe-stacks --stack-name ${INFRA_ID}-control-plane --query "Stacks[].StackStatus"
----

. Get the stack output
+
[source,bash]
----
aws cloudformation describe-stacks --stack-name ${INFRA_ID}-control-plane --query "Stacks[].Outputs" --output text
----
+
.Sample Output:
[source,text]
----
The control-plane node private IP addresses.    PrivateIPs      10.0.52.66,10.0.48.249,10.0.61.151
----

=== Verify Bootstrap Initialization

. With your `control plane` nodes now created, switch back to your other SSH window and watch the `bootkube` process.
+
As of OpenShift 4.11, it will check approximately every 5 seconds for the `etcd` cluster to be healthy before it continues the bootstrapping process.

. Depending on when your `control plane` nodes booted and formed the etcd cluster, it could take several minutes for the bootstrap process to continue and finish.
If you do not see your `etcd` cluster showing as healthy in the `bootkube` logs within 10-20 minutes, your cluster is not healthy and you probably did something wrong.
+
While you are waiting for this, take a minute to revisit the image mirroring you did earlier.
You added an `imageContentSources` into your `install-config.yaml` file, but what did that actually do?

. On your `bootstrap` node, run the following command to see the images that have been pulled down as part of the bootstrapping process.
+
[source,sh]
----
sudo podman images
----
+
.Sample Output
[source,sh,options="nowrap"]
----
REPOSITORY                                       TAG         IMAGE ID      CREATED      SIZE
utilityvm.smp1235.internal:5000/ocp4/openshift4  <none>      733678a2b009  4 weeks ago  359 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      4a88fde92e3f  4 weeks ago  377 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      782c3d4724ed  5 weeks ago  783 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      0decbc798b93  5 weeks ago  371 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      123359cdec40  5 weeks ago  464 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      dc3e9fc64bcf  5 weeks ago  370 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      80ea63336b20  5 weeks ago  367 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      3129b0d0a7a5  5 weeks ago  449 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      cb1bc19cdf96  5 weeks ago  369 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      4a19d3445923  5 weeks ago  292 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      506ff68e5ba0  5 weeks ago  377 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      f555abfd6f2b  5 weeks ago  374 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      c3174801cbfe  5 weeks ago  373 MB
quay.io/openshift-release-dev/ocp-v4.0-art-dev   <none>      b362ea908ab6  5 weeks ago  336 MB
----

. Notice that, even though you specified your `imageContentSources` policy to pull from `utilityvm.$GUID.internal`, these are still showing as coming from `quay.io`.
This is because OpenShift does not rewrite pod specs, manifests, or anything else depending on where you are hosting your images.
It simply tells the container engine what these alternate sourecs are.
Verify what yours are set to.
+
[source,sh]
----
sudo cat /etc/containers/registries.conf
----
+
.Sample Output
[source,sh]
----
[[registry]]
location = "quay.io/openshift-release-dev/ocp-release"
insecure = false
mirror-by-digest-only = true

[[registry.mirror]]
location = "utilityvm.smp1235.internal:5000/ocp4/openshift4"
insecure = false


[[registry]]
location = "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
insecure = false
mirror-by-digest-only = true

[[registry.mirror]]
location = "utilityvm.smp1235.internal:5000/ocp4/openshift4"
insecure = false
----
+
* There are a few things to note in this configuration:
** This mirroring will *only* work when the image reference includes a digest.
Referencing images by digest is the only way to guarantee you are getting the _exact_ version you want.
** If you use tags, it will still try to pull from the original location.
While tags are more user friendly, they can have any version of the image associated with them and could be different between different registries.
** When `registry.mirror` is specified, they are tried in the order listed.
The "original" is tried last.
In this example, `utilityvm.$GUID.internal` will be tried first.
+
* This functionality is provided by the *containers/image* library.
The OpenShift installer and resulting cluster-wide configuration take care of setting this up for you.
The key item to focus on is making sure the images have all been properly mirrored and are accessible from your local container registry.
Once you provide the correct `imageContentSources` to the installer, you don't need to look at this any longer.
If you want to see some more details about the containers/image library, read some of the link:https://github.com/containers/image/blob/master/docs/containers-registries.conf.5.md#remapping-and-mirroring-registries[upstream documentation^].

. Continue to watch the `bootkube` logs on your `bootstrap` node.
+
[source,sh]
----
journalctl -b -f -u release-image.service -u bootkube.service
----

. When you see the following entries in the log file, your cluster is done bootstrapping.
+
[source,text]
----
[....]

Feb 19 02:09:52 ip-10-0-12-208 bootkube.sh[9273]: I0219 02:09:52.339635       1 waitforceo.go:64] Cluster etcd operator bootstrapped successfully
Feb 19 02:09:52 ip-10-0-12-208 bootkube.sh[9273]: I0219 02:09:52.340500       1 waitforceo.go:58] cluster-etcd-operator bootstrap etcd
Feb 19 02:09:52 ip-10-0-12-208 bootkube.sh[9273]: bootkube.service complete
----

. Disconnect from the bootstrap VM by hitting `Ctrl-C` followed by `Ctrl-D` (or `exit`).

. Run `openshift-install` to progress the installation process.
+
[source,bash]
----
openshift-install wait-for bootstrap-complete --dir=$HOME/aws-upi --log-level=info
----
+
.Sample Output:
[source,text]
----
INFO Waiting up to 20m0s (until 2:53PM) for the Kubernetes API at https://api.cluster-smp1235.sandbox2813.opentlc.com:6443... 
INFO API v1.24.6+5157800 up                       
INFO Waiting up to 30m0s (until 3:03PM) for bootstrapping to complete... 
INFO It is now safe to remove the bootstrap resources 
INFO Time elapsed: 0s 
----

. Now that bootstrapping has completed your control plane is fully self sufficient and you can delete the bootstrap VM.
+
You can simply delete the bootstrap cloud formation stack that you created before:
+
[source,sh]
----
aws cloudformation delete-stack --stack-name ${INFRA_ID}-bootstrap
----

=== Connect to the Control Plane

. At this point, you can interact with your cluster.
You'll need to set credentials for yourself first.
+
[source,sh]
----
ansible localhost -m lineinfile -a 'path=$HOME/.bashrc regexp="^export KUBECONFIG" line="export KUBECONFIG=$HOME/aws-upi/auth/kubeconfig"'

source $HOME/.bashrc
----

. Check the current state of the `clusterversion`.
Take note that the *STATUS* message will be different depending on when you run this.
+
[source,sh]
----
oc get clusterversion
----
+
.Sample Output
[source,sh,options="nowrap"]
----
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version             False       True          22m     Working towards 4.11.11: 776 of 803 done (96% complete)
----
+
. Run the command until you see (you don't *really* have to wait until you see this - you would eventually - feel free to continue the lab):
+
[source,sh]
----
oc get clusterversion
----
+
.Sample Output
[source,sh,options="nowrap"]
----
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version             False       True          31m     Unable to apply 4.11.11: some cluster operators have not yet rolled out
----
+
NOTE: This is OK. Some cluster operators need to run on worker nodes. But you have not created any worker nodes yet. In the following section you will unblock the deployment.

. Check the current state of the `clusteroperators`.
Keep executing the command and watch the fields change value.
Look up the purpose of clusteroperators that you are unfamiliar with.
At this phase of the installation, should they be AVAILABLE?
PROGRESSING?
DEGRADED?
How of then has it been SINCE they were fist discovered?
+
[source,sh]
----
oc get clusteroperators
----
+
.Sample Output
[source,sh,options="nowrap"]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                                       False       False         True       13m
cloud-credential                           4.11.11   True        False         False      20m
cluster-autoscaler                         4.11.11   True        False         False      12m
config-operator                            4.11.11   True        False         False      13m
console                                    4.11.11   Unknown     True          False      7m
csi-snapshot-controller                    4.11.11   True        False         False      13m
dns                                        4.11.11   True        False         False      12m
etcd                                       4.11.11   True        False         False      11m
image-registry                                       False       True          False      7m38s
ingress                                              False       True          True       12m
insights                                   4.11.11   True        False         False      13m
kube-apiserver                             4.11.11   True        True          False      11m
kube-controller-manager                    4.11.11   True        False         False      11m
kube-scheduler                             4.11.11   True        False         False      10m
kube-storage-version-migrator              4.11.11   False       False         False      13m
machine-api                                4.11.11   True        False         False      3m51s
machine-approver                           4.11.11   True        False         False      12m
machine-config                             4.11.11   True        False         False      11m
marketplace                                4.11.11   True        False         False      11m
monitoring                                           False       True          True       2m13s
network                                    4.11.11   True        False         False      13m
node-tuning                                4.11.11   True        False         False      13m
openshift-apiserver                        4.11.11   True        False         False      7m28s
openshift-controller-manager               4.11.11   True        False         False      11m
openshift-samples                          4.11.11   True        False         False      6m48s
operator-lifecycle-manager                 4.11.11   True        False         False      12m
operator-lifecycle-manager-catalog         4.11.11   True        False         False      12m
operator-lifecycle-manager-packageserver   4.11.11   True        False         False      7m15s
service-ca                                 4.11.11   True        False         False      13m
storage                                    4.11.11   True        False         False      12m
----
+
[NOTE]
Not all of the cluster operators will be able to deploy until you have added regular `worker` nodes. There are certain cluster workloads that cannot or will not deploy to `control plane` nodes.
If you see similar output with both of the previous two commands, you are ready for the next section - creating Workers.

Your cluster is now almost up and running!

=== Create Workers

Almost to the end!
Your cluster is built and running, but it doesn't have any workers.
Workers are required for some of the cluster operators to deploy their workloads.
Without workers, your cluster will not finish installing.
In an IPI install, this would happen automatically, but like all other steps in this UPI lab, you have to create the worker nodes manually.

. Add the certificates from the worker.ign file to the worker parameters file.
.. Copy your master cert to your clipboard
+
[source,bash]
----
jq .ignition.security.tls.certificateAuthorities[0].source ~/aws-upi/worker.ign
----
.. Paste your master cert to replace the `XXXX` in the file `~/resources/aws_upi_worker_parameters.json` and examine the other parameters in the file.
+
.Sample Output:
[source,text]
----
  {
    "ParameterKey": "CertificateAuthorities",
    "ParameterValue": "XXXX"
  },
----

. Execute the create stack command twice to create two worker nodes.
.. Create the first worker stack:
+
[source,bash]
----
aws cloudformation create-stack --stack-name ${INFRA_ID}-worker-1 \
    --template-body file://~/resources/aws_upi_worker_template.yaml \
    --parameters file://~/resources/aws_upi_worker_parameters.json
----

.. Create the second worker stack
+
[source,bash]
----
aws cloudformation create-stack --stack-name ${INFRA_ID}-worker-2 \
    --template-body file://~/resources/aws_upi_worker_template.yaml \
    --parameters file://~/resources/aws_upi_worker_parameters.json
----

. Watch the first stack until the stack status shows *CREATE_COMPLETE*:
+
[source,sh]
----
watch -n 5 aws cloudformation describe-stacks --stack-name ${INFRA_ID}-worker-1 --query "Stacks[].StackStatus"
----

. Double check that the second stack completed successfully as well. If it still shows in progress wait until it also shows complete.
+
[source,sh]
----
aws cloudformation describe-stacks --stack-name ${INFRA_ID}-worker-2 --query "Stacks[].StackStatus"
----

. Get the stack outputs for the two stacks.
+
[source,bash]
----
aws cloudformation describe-stacks --stack-name ${INFRA_ID}-worker-1 --query "Stacks[].Outputs" --output text

aws cloudformation describe-stacks --stack-name ${INFRA_ID}-worker-2 --query "Stacks[].Outputs" --output text
----
+
.Sample Output
[source,texinfo]
----
The compute node private IP address.	PrivateIP	10.0.55.97
The compute node private IP address.	PrivateIP	10.0.60.82
----

. Because you are adding these `worker` nodes manually, they will not automatically be able to bootstrap and join the cluster.
You will have to approve them.
This is done so that not any random server that happens to have access on your network can join your OpenShift cluster.
Like the `control plane` nodes, the `worker` nodes will pull their `RHCOS` ignition configurations from the control plane.
Specifically, they will pull it from the `machine-config-server`, which will be discussed later. The bootstrap location is specified using the *IgnitionLocation* parameter in the file above.

[#approveworkers]
. To allow your new workers into the cluster, you have to approve their _certificate signing request_ or `CSR`.
Watch for the `CSR` to come in.
There will be one for each `worker` *machine* to bootstrap and there will be one for each worker *node*. Keep executing the command below until you see *two* certificate signing requests with condition *Pending*.
+
[source,sh]
----
oc get csr
----
+
.Sample Output
[source,sh,options="nowrap"]
----
NAME        AGE     SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-4rxq5   2m26s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-5r5f9   23m     kubernetes.io/kubelet-serving                 system:node:ip-10-0-60-37.us-east-2.compute.internal                        Approved,Issued
csr-bsmrr   23m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Approved,Issued
csr-ftls7   23m     kubernetes.io/kubelet-serving                 system:node:ip-10-0-56-201.us-east-2.compute.internal                       Approved,Issued
csr-lw6mf   23m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Approved,Issued
csr-mbrms   23m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Approved,Issued
csr-rn9kx   23m     kubernetes.io/kubelet-serving                 system:node:ip-10-0-54-215.us-east-2.compute.internal                       Approved,Issued
csr-xrl56   2m46s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
----
<1> a new machine-config for the node to come later

. Once you see the `CSR`, you should not just approve it.
You should inspect it to make sure it is coming from a source you trust. Change the describe command pointing to your CSRs in pending state.
+
[source,sh]
----
oc describe csr csr-4rxq5
----
+
.Sample Output
[source,sh,options="nowrap"]
----
Name:               csr-4rxq5
Labels:             <none>
Annotations:        <none>
CreationTimestamp:  Sat, 20 Feb 2021 17:29:28 +0000
Requesting User:    system:serviceaccount:openshift-machine-config-operator:node-bootstrapper <1>
Signer:             kubernetes.io/kube-apiserver-client-kubelet
Status:             Pending
Subject:
         Common Name:    system:node:ip-10-0-60-82.us-east-2.compute.internal
         Serial Number:
         Organization:   system:nodes
Events:  <none>
----
<1> Note that it is the machine config operator that is requesting the cert signing.

. When you are satisifed that the `CSR` is coming from a trusted node, you can approve it to complete its bootstrapping process.
+
[source,sh]
----
oc adm certificate approve csr-4rxq5
----
+
.Sample Output
[source,text]
----
certificatesigningrequest.certificates.k8s.io/csr-4rxq5 approved
----

. Repeat these steps with the `CSR` for your other `worker` node. Verify both CSR are now approved.

. Once the `worker` nodes have finished bootstrapping, they will ask to join the cluster *as nodes*.
This will come as another set of `CSR` that you will need to approve. Once again wait until you have two CSRs in *Pending* condition.
+
[source,sh]
----
oc get csr
----
+
.Sample Output
[source,sh,options="nowrap"]
----
NAME        AGE     SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-4rxq5   5m32s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Approved,Issued
csr-5r5f9   26m     kubernetes.io/kubelet-serving                 system:node:ip-10-0-60-37.us-east-2.compute.internal                        Approved,Issued
csr-bsmrr   26m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Approved,Issued
csr-ftls7   26m     kubernetes.io/kubelet-serving                 system:node:ip-10-0-56-201.us-east-2.compute.internal                       Approved,Issued
csr-h2sv6   47s     kubernetes.io/kubelet-serving                 system:node:ip-10-0-60-82.us-east-2.compute.internal                        Pending <1>
csr-ksv5l   18s     kubernetes.io/kubelet-serving                 system:node:ip-10-0-55-97.us-east-2.compute.internal                        Pending <1>
csr-lw6mf   26m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Approved,Issued
csr-mbrms   26m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Approved,Issued
csr-rn9kx   26m     kubernetes.io/kubelet-serving                 system:node:ip-10-0-54-215.us-east-2.compute.internal                       Approved,Issued
csr-xrl56   5m52s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Approved,Issued
----
<1> a new *node*

. Look at one of these new `CSR` to see the difference.
+
[source,sh]
----
oc describe csr csr-h2sv6
----
+
.Sample Output
[source,sh]
----
Name:               csr-h2sv6
Labels:             <none>
Annotations:        <none>
CreationTimestamp:  Sat, 20 Feb 2021 17:34:13 +0000
Requesting User:    system:node:ip-10-0-60-82.us-east-2.compute.internal <1>
Signer:             kubernetes.io/kubelet-serving
Status:             Pending
Subject:
  Common Name:    system:node:ip-10-0-60-82.us-east-2.compute.internal
  Serial Number:
  Organization:   system:nodes
Subject Alternative Names:
         DNS Names:     ip-10-0-60-82.us-east-2.compute.internal
         IP Addresses:  10.0.60.82
Events:  <none>
----
<1> Note that this is the *node* user, not the machine config.

. If this matches what you expect, approve the `CSR`s.
+
[source,sh]
----
oc adm certificate approve csr-h2sv6 csr-ksv5l
----
. Verify that all CSRs are now approved.
. Inspect your nodes to ensure that you now have a total of five in a `Ready` state.
Three of them should be `master` and two of them should be `worker`.
+
[source,sh]
----
oc get nodes
----
+
.Sample Output
[source,sh]
----
NAME                                        STATUS   ROLES    AGE     VERSION
ip-10-0-54-215.us-east-2.compute.internal   Ready    master   28m     v1.19.0+e49167a
ip-10-0-55-97.us-east-2.compute.internal    Ready    worker   117s    v1.19.0+e49167a
ip-10-0-56-201.us-east-2.compute.internal   Ready    master   28m     v1.19.0+e49167a
ip-10-0-60-37.us-east-2.compute.internal    Ready    master   28m     v1.19.0+e49167a
ip-10-0-60-82.us-east-2.compute.internal    Ready    worker   2m27s   v1.19.0+e49167a
----

=== Finish the installation

. Check your cluster version to see the successful deployment. It may take a few minutes for the cluster to continue deploying after the additional worker nodes have been added. Eventually the cluster operators will roll out successfully and your cluster will finish installing.
Keep checking your Cluster Operators until all operators are available, not progressing and not degraded. Because the `openshift-kube-apiserver` clusteroperators are updating this may take 15 or more minutes (one api server takes about 5 minutes to update after a change).
+
[source,sh]
----
watch -n 5 oc get co
----
+
.Sample Output
[source,texinfo]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             4.11.11   True        False         False      36m
cloud-credential                           4.11.11   True        False         False      44m
cluster-autoscaler                         4.11.11   True        False         False      35m
config-operator                            4.11.11   True        False         False      36m
console                                    4.11.11   True        False         False      4m
csi-snapshot-controller                    4.11.11   True        False         False      36m
dns                                        4.11.11   True        False         False      35m
etcd                                       4.11.11   True        False         False      34m
image-registry                             4.11.11   True        False         False      10m
ingress                                    4.11.11   True        False         False      9m50s
insights                                   4.11.11   True        False         False      36m
kube-apiserver                             4.11.11   True        False         False      34m
kube-controller-manager                    4.11.11   True        False         False      34m
kube-scheduler                             4.11.11   True        False         False      33m
kube-storage-version-migrator              4.11.11   True        False         False      10m
machine-api                                4.11.11   True        False         False      26m
machine-approver                           4.11.11   True        False         False      35m
machine-config                             4.11.11   True        False         False      34m
marketplace                                4.11.11   True        False         False      34m
monitoring                                 4.11.11   True        False         False      9m8s
network                                    4.11.11   True        False         False      36m
node-tuning                                4.11.11   True        False         False      36m
openshift-apiserver                        4.11.11   True        False         False      30m
openshift-controller-manager               4.11.11   True        False         False      35m
openshift-samples                          4.11.11   True        False         False      29m
operator-lifecycle-manager                 4.11.11   True        False         False      35m
operator-lifecycle-manager-catalog         4.11.11   True        False         False      35m
operator-lifecycle-manager-packageserver   4.11.11   True        False         False      30m
service-ca                                 4.11.11   True        False         False      36m
storage                                    4.11.11   True        False         False      35m
----

. Finally check that the cluster version now shows as available.
+
[source,sh]
----
oc get clusterversion
----
+
.Sample Output
[source,sh,options="nowrap"]
----
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.11.11   True        False         1s      Cluster version is 4.11.11 
----

. Once the cluster operators have deployed successfully use `openshift-install` to finish the installation. Once the clusterversion shows as finished this command should return very quickly.
+
[source,sh,options="nowrap"]
----
openshift-install wait-for install-complete --dir=$HOME/aws-upi
----
+
.Sample Output
[source,texinfo]
----
INFO Waiting up to 40m0s (until 3:43PM) for the cluster at https://api.cluster-smp1235.sandbox2813.opentlc.com:6443 to initialize... 
INFO Waiting up to 10m0s (until 3:13PM) for the openshift-console route to be created... 
INFO Install complete!                            
INFO To access the cluster as the system:admin user when using 'oc', run 
INFO     export KUBECONFIG=/home/lab-user/aws-upi/auth/kubeconfig 
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.cluster-smp1235.sandbox2813.opentlc.com 
INFO Login to the console with user: "kubeadmin", and password: "pgzUU-CTwQK-LLSgS-puZV9" 
INFO Time elapsed: 0s 
----

You now have a fully functional cluster with worker nodes ready to run workloads.

Take note of the above information.
It is how you will access your new cluster.
You will need some of this for the next lab.

*Congratulations!*

You have fully deployed an OpenShift 4 cluster using the UPI method in a disconnected environment.

// Cleanup section pending.
== Appendicies

The following sections are *not required* to finish the lab and be prepared for the next lab.

=== Appendix 1: Automated Solving and Resetting

====

WARNING: THE FOLLOWING STEPS WILL ERASE ALL YOUR WORK

Use the steps here to either solve or reset this lab.

Solving the lab will both reset (delete) whatever you currently have deployed and then do a full UPI disconnected installation.
Resetting the lab will only reset and cleanup anything you currently have deployed so you can start fresh.

NOTE: Resetting the lab requires the file `$HOME/aws-upi/metadata.json` to be present.

. On your `bastion`, ensure you have the following:
* Your OpenShift pull secret added to the `$HOME/ocp_pullsecret.json` file.
You do not need the `merged_pullsecret.json` as the solver will create it for you.
* Your AWS credentials (`$HOME/.aws/credentials`) as supplied in the environment deploy email

. To solve the lab
+
[source,options="nowrap"]
----
solve_lab ocp4_advanced_deployment 03_1aws
----
+
.. Open a new terminal to view logs in: `/tmp/ansible_solving.log`
.. When installation is complete, access your cluster by loading in the `kube/config` file.
Simply execute `source ~/.bashrc`

. To *reset* the lab, and delete all your work, run the following command
+
[source,options="nowrap"]
----
reset_lab ocp4_advanced_deployment 03_1aws
----
.. Open a new terminal to view logs in: `/tmp/ansible_resetting.log`
====
